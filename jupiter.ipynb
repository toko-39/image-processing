{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5245f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ワークスペース内で発見: c:\\Users\\tokot\\code\\image-processing\\all\\CIFAR-10_data\\cifar-10-batches-py\n",
      "\n",
      "--- 訓練モード実行中 ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (100,3468) and (3072,100) not aligned: 3468 (dim 1) != 3072 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 415\u001b[39m\n\u001b[32m    412\u001b[39m train_images_col = batch_image  \u001b[38;5;66;03m# shape: (B, 3072)\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# 順伝播を実行\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m output_probabilities, hidden_layer_input, hidden_layer_output = \u001b[43mforward_propagation_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_images_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_selection\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[38;5;66;03m# one-hot labelsを取得\u001b[39;00m\n\u001b[32m    419\u001b[39m one_hot_labels = get_one_hot_label(batch_labels, output_layer_size)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 247\u001b[39m, in \u001b[36mforward_propagation_train\u001b[39m\u001b[34m(input_vector, weight1, bias1, weight2, bias2, ignore_number)\u001b[39m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_propagation_train\u001b[39m(input_vector, weight1, bias1, weight2, bias2, ignore_number):\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     hidden_layer_input = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m + bias1\n\u001b[32m    248\u001b[39m     hidden_layer_output = ReLU(hidden_layer_input)\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ignore_number:\n",
      "\u001b[31mValueError\u001b[39m: shapes (100,3468) and (3072,100) not aligned: 3468 (dim 1) != 3072 (dim 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ensure_cifar(data_dir=\"all/cifar-10-batches-py\"):\n",
    "    data_dir = Path(data_dir)\n",
    "    # 既に指定パスに存在すれば返す\n",
    "    if (data_dir / \"data_batch_1\").exists():\n",
    "        print(\"CIFAR-10 データは既に存在します:\", data_dir)\n",
    "        return str(data_dir)\n",
    "\n",
    "    # ワークスペース内を探索して見つかれば返す\n",
    "    ws_root = Path(r\"c:\\Users\\tokot\\code\\image-processing\")\n",
    "    for p in ws_root.rglob(\"data_batch_1\"):\n",
    "        print(\"ワークスペース内で発見:\", p.parent)\n",
    "        return str(p.parent)\n",
    "\n",
    "    # 見つからなければダウンロードして展開\n",
    "    print(\"CIFAR-10 が見つかりません。ダウンロードを開始します...\")\n",
    "    dest = ws_root / \"cifar-10-python.tar.gz\"\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "        print(\"ダウンロード完了:\", dest)\n",
    "        with tarfile.open(dest, \"r:gz\") as tf:\n",
    "            tf.extractall(path=ws_root)\n",
    "        extracted = ws_root / \"cifar-10-batches-py\"\n",
    "        if (extracted / \"data_batch_1\").exists():\n",
    "            print(\"展開完了:\", extracted)\n",
    "            return str(extracted)\n",
    "        raise FileNotFoundError(\"展開後に期待ファイルが見つかりません\")\n",
    "    except Exception as e:\n",
    "        print(\"取得失敗:\", e)\n",
    "        raise\n",
    "\n",
    "def load_cifar10(data_dir):\n",
    "    \"\"\" (N,3072) の train/test とラベルを返す（float32 0-1 正規化） \"\"\"\n",
    "    train_data = None\n",
    "    train_labels = []\n",
    "    data_dir = str(data_dir)\n",
    "    for i in range(1, 6):\n",
    "        path = os.path.join(data_dir, f\"data_batch_{i}\")\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(path)\n",
    "        with open(path, \"rb\") as f:\n",
    "            data_dict = pickle.load(f, encoding=\"bytes\")\n",
    "            if train_data is None:\n",
    "                train_data = data_dict[b\"data\"]\n",
    "            else:\n",
    "                train_data = np.vstack((train_data, data_dict[b\"data\"]))\n",
    "            train_labels.extend(data_dict[b\"labels\"])\n",
    "    test_path = os.path.join(data_dir, \"test_batch\")\n",
    "    with open(test_path, \"rb\") as f:\n",
    "        data_dict = pickle.load(f, encoding=\"bytes\")\n",
    "        test_data = data_dict[b\"data\"]\n",
    "        test_labels = data_dict[b\"labels\"]\n",
    "\n",
    "    train_data = np.array(train_data, dtype=np.float32) / 255.0\n",
    "    test_data = np.array(test_data, dtype=np.float32) / 255.0\n",
    "    train_labels = np.array(train_labels, dtype=np.int64)\n",
    "    test_labels = np.array(test_labels, dtype=np.int64)\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "def padding_data(train_images, test_images, pad=1):\n",
    "    \"\"\"CIFAR-10 の1次元ベクトル配列を画像形状に戻しパディングを付与して返す。\n",
    "    引数:\n",
    "      train_images: (N, 3072)\n",
    "      test_images:  (M, 3072)\n",
    "      pad: パディング幅（ピクセル）\n",
    "    戻り値:\n",
    "      padded_train_images: (N, 32+2*pad, 32+2*pad, 3)\n",
    "      padded_test_images:  (M, 32+2*pad, 32+2*pad, 3)\n",
    "    \"\"\"\n",
    "    # (N, 3072) -> (N, 3, 32, 32) -> (N, 32, 32, 3)\n",
    "    train_imgs = train_images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_imgs = test_images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    padded_train = np.pad(\n",
    "        train_imgs,\n",
    "        ((0, 0), (pad, pad), (pad, pad), (0, 0)),\n",
    "        mode=\"constant\",\n",
    "        constant_values=0.0,\n",
    "    )\n",
    "    padded_test = np.pad(\n",
    "        test_imgs,\n",
    "        ((0, 0), (pad, pad), (pad, pad), (0, 0)),\n",
    "        mode=\"constant\",\n",
    "        constant_values=0.0,\n",
    "    )\n",
    "\n",
    "    return padded_train, padded_test\n",
    "\n",
    "data_dir = ensure_cifar(\"all/cifar-10-batches-py\")\n",
    "train_images, train_labels, test_images, test_labels = load_cifar10(data_dir)\n",
    "padded_train_images, padded_test_images = padding_data(train_images, test_images, pad=1)\n",
    "\n",
    "# レイヤーの次元数を修正（CIFAR-10用）\n",
    "input_size = 3072  # 32x32x3（カラー画像）\n",
    "hidden_layer_size = 100  # 中間層は同じ\n",
    "output_layer_size = 10   # CIFAR-10も10クラス\n",
    "\n",
    "def im2col(padding_data, filter_size, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Im2col変換を行う関数 (NumPyを用いた簡略版ロジック)\n",
    "    \n",
    "    input_data: (N, H, W, C) の4次元配列\n",
    "    \"\"\"\n",
    "    N, H, W, C = padding_data.shape\n",
    "    \n",
    "    # 出力特徴マップのサイズ計算\n",
    "    out_h = (H - filter_size) // stride + 1\n",
    "    out_w = (W - filter_size) // stride + 1\n",
    "    \n",
    "    # パッチ抽出のためのループとインデックス計算\n",
    "    col = np.zeros((N, out_h, out_w, filter_size, filter_size, C))\n",
    "\n",
    "    # パッチのインデックスを計算して、colに格納していく (非常に複雑な処理)\n",
    "    # i, j は出力特徴マップ上の座標\n",
    "    for i in range(out_h):\n",
    "        i_max = i * stride + filter_size\n",
    "        for j in range(out_w):\n",
    "            j_max = j * stride + filter_size\n",
    "            \n",
    "            # パッチを抽出 (例: input_data[:, i*stride:i_max, j*stride:j_max, :] )\n",
    "            col[:, i, j, :, :, :] = padding_data[:, i * stride:i_max, j * stride:j_max, :]\n",
    "            \n",
    "    # 形状を (N * out_h * out_w, filter_h * filter_w * C) に変換（行列乗算の形式）\n",
    "    # ただし、添付図の形式 (filter_h * filter_w * C, N * out_h * out_w) にするためには転置が必要\n",
    "    col = col.transpose(0, 4, 5, 3, 1, 2).reshape(N * out_h * out_w, -1).T\n",
    "    \n",
    "    return col\n",
    "\n",
    "def set_filter_weights():\n",
    "    \"\"\"\n",
    "    フィルタの重み行列 W を He初期化で初期化する関数\n",
    "    戻り値:\n",
    "      W: (K, R*R*ch) の形状の重み行列\n",
    "    \"\"\"\n",
    "    # フィルタのパラメータ設定\n",
    "    K = 32  # フィルタ枚数\n",
    "    R = 3   # フィルタサイズ\n",
    "    ch = 3  # 入力チャネル数\n",
    "\n",
    "    # 1. He初期化に必要な「入力ノード数」の計算\n",
    "    # 入力ノード数 = 1つの出力特徴マップの要素を計算するのに使われる入力データ数\n",
    "    #              = フィルタサイズ(R*R) * 入力チャネル数(ch)\n",
    "    input_node_count = R * R * ch  # 3 * 3 * 3 = 27\n",
    "\n",
    "    # 2. 標準偏差の計算 (He初期化)\n",
    "    std_dev = np.sqrt(2.0 / input_node_count)\n",
    "\n",
    "    # 3. 重み行列 W の初期化\n",
    "    # 形状は (K, R*R*ch) = (32, 27)\n",
    "    W = std_dev * np.random.randn(K, R * R * ch)\n",
    "    \n",
    "    return W, R\n",
    "\n",
    "def set_biases():\n",
    "    \"\"\"\n",
    "    バイアスベクトル b を初期化する関数\n",
    "    戻り値:\n",
    "      b: (K,) の形状のバイアスベクトル\n",
    "    \"\"\"\n",
    "    K = 32  # フィルタ枚数\n",
    "    b = np.zeros(K)  # バイアスはゼロで初期化\n",
    "\n",
    "    # 1. バイアスベクトルを (K, 1) に整形\n",
    "    b_vector = b.reshape(-1, 1)\n",
    "    \n",
    "    return b_vector\n",
    "\n",
    "def get_shuffled_index(arr):\n",
    "    index_arr = np.arange(len(arr)) # 0から始まるインデックスの配列を作成\n",
    "    np.random.shuffle(index_arr) # インデックスをシャッフル\n",
    "    return index_arr\n",
    "\n",
    "def get_batch(random_index):\n",
    "    \"\"\"ミニバッチ画像を平坦化して返す（全結合層向け）\n",
    "    パディングを除いた中心部分（32x32）のみを使用\n",
    "    \"\"\"\n",
    "    batch_images = padded_train_images[random_index]            # (B, H, W, C)\n",
    "    # パディング除去（中心の32x32を切り出し）\n",
    "    batch_images = batch_images[:, 1:-1, 1:-1, :]             # パディング1なので両端1ピクセルずつ除去\n",
    "    batch_images_flat = batch_images.reshape(batch_images.shape[0], -1)  # (B, 3072)\n",
    "    batch_labels = train_labels[random_index]\n",
    "    return batch_images_flat, batch_labels\n",
    "\n",
    "def get_one_hot_label(batch_labels, output_layer_size):\n",
    "    one_hot_labels = np.zeros((batch_labels.size, output_layer_size)) # ゼロで満たされた配列を作成\n",
    "    one_hot_labels[np.arange(batch_labels.size), batch_labels] = 1 # 各行の、正解ラベルに対応するインデックスを1にする\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "np.random.seed(777) # シードを固定\n",
    "\n",
    "# 重みとバイアスを正規分布で初期化\n",
    "\n",
    "is_load = str(input('ロードしますか？ yes or no:  '))\n",
    "if is_load == 'yes' :\n",
    "    loaded_data = np.load('CIFAR-10_parameter.npz')\n",
    "    weight1 = loaded_data['weight1']\n",
    "    bias1 = loaded_data['bias1']\n",
    "    weight2 = loaded_data['weight2']\n",
    "    bias2 = loaded_data['bias2']\n",
    "else:\n",
    "    # 全結合層の初期化（CIFAR 用に input_size=3072 を使用）\n",
    "    weight1 = np.random.normal(loc=0.0, scale=np.sqrt(1 / input_size), size=(hidden_layer_size, input_size))  # (100,3072)\n",
    "    bias1 = np.zeros((hidden_layer_size,))  # (100,)\n",
    "    # 第2層（中間層 -> 出力層）\n",
    "    weight2 = np.random.normal(loc=0.0, scale=np.sqrt(1 / hidden_layer_size), size=(output_layer_size, hidden_layer_size)) # (10,100)\n",
    "    bias2 = np.random.normal(loc=0.0, scale=np.sqrt(1 / hidden_layer_size), size=output_layer_size) # (10,)\n",
    "def ReLU(arr):\n",
    "    \"\"\"課題4-1 ReLU活性化関数\"\"\"\n",
    "    new_arr = np.where(arr > 0, arr, 0)\n",
    "    return new_arr\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    ソフトマックス関数（オーバーフロー対策版）\n",
    "    各要素を0から1の間の確率に変換\n",
    "    \"\"\"\n",
    "    alpha = np.max(x, axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x - alpha)\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# --- 順伝播の実行(重みを更新) ---\n",
    "def forward_propagation(input_vector, weight1, bias1, weight2, bias2):\n",
    "    \n",
    "    # 中間層の計算: 活性化関数の入力　 (バッチサイズ, 784) @ (784, 100) -> (バッチサイズ, 100)\n",
    "    hidden_layer_input = np.dot(input_vector, weight1.T) + bias1\n",
    "    \n",
    "    # 中間層の出力: 活性化関数を適用\n",
    "    # hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    hidden_layer_output = ReLU(hidden_layer_input)\n",
    "    \n",
    "    # 出力層の計算: 活性化関数の入力　(バッチサイズ, 100) @ (100, 10) -> (バッチサイズ, 10)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weight2.T) + bias2\n",
    "    \n",
    "    # 出力層の出力: ソフトマックスを適用して確率を算出\n",
    "    final_output = softmax(output_layer_input)\n",
    "    \n",
    "    return final_output, hidden_layer_input, hidden_layer_output # 4-1 hidden_layer_inputを出力に追加\n",
    "\n",
    "def forward_propagation_train(input_vector, weight1, bias1, weight2, bias2, ignore_number):\n",
    "    \n",
    "    hidden_layer_input = np.dot(input_vector, weight1.T) + bias1\n",
    "    hidden_layer_output = ReLU(hidden_layer_input)\n",
    "    for index in ignore_number:\n",
    "        hidden_layer_output[:, index] = 0\n",
    "    output_layer_input = np.dot(hidden_layer_output, weight2.T) + bias2\n",
    "    final_output = softmax(output_layer_input)\n",
    "    return final_output, hidden_layer_input, hidden_layer_output  \n",
    "\n",
    "def forward_propagation_test(input_vector, weight1, bias1, weight2, bias2, ignore_number):\n",
    "    \n",
    "    hidden_layer_input = np.dot(input_vector, weight1.T) + bias1\n",
    "    hidden_layer_output = ReLU(hidden_layer_input)\n",
    "    hidden_layer_output *= 1 - (len(ignore_number) / hidden_layer_size)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weight2.T) + bias2\n",
    "    final_output = softmax(output_layer_input)\n",
    "    return final_output, hidden_layer_input, hidden_layer_output     \n",
    "\n",
    "def get_predicted_class(output_probabilities):\n",
    " # 出力された確率から最も高い確率を持つクラス（予測結果）を取得\n",
    "    if output_probabilities.ndim == 1:\n",
    "        return np.argmax(output_probabilities)\n",
    "    else:\n",
    "        return np.argmax(output_probabilities, axis=1)\n",
    "\n",
    "def get_cross_entropy_error(y_pred, y_true):\n",
    "    \n",
    "    delta = 1e-7\n",
    "    \n",
    "    loss = -np.sum(y_true * np.log(y_pred + delta)) # logの中身が0にならないようにdeltaを導入\n",
    "    \n",
    "    # ミニバッチサイズBで割って平均を求める\n",
    "    batch_size = y_pred.shape[0]\n",
    "    \n",
    "    cross_entropy_error = loss / batch_size\n",
    "    \n",
    "    return cross_entropy_error\n",
    "\n",
    "def backward_propagation_and_update(batch_image_vector, hidden_layer_input, hidden_layer_output, output_probabilities, one_hot_labels, \n",
    "                                    weight1, bias1, weight2, bias2, learning_rate):\n",
    "    \"\"\"\n",
    "    逆伝播法を用いて勾配を計算し、全パラメータを更新する関数。\n",
    "    更新後のパラメータを返す。 4-1 ReLUにより、hidden_layer_inputも受け取る。\n",
    "    \"\"\"\n",
    "    current_batch_size = batch_image_vector.shape[0]\n",
    "    \n",
    "    # --- 逆伝播 ---\n",
    "    dEn_dak = (output_probabilities - one_hot_labels) / current_batch_size\n",
    "    dEn_dX = np.dot(dEn_dak, weight2)\n",
    "    dEn_dW_1 = np.dot(dEn_dak.T, hidden_layer_output)\n",
    "    dEn_db_1 = np.sum(dEn_dak, axis = 0)\n",
    "    # dEn_dX_sig = dEn_dX * (hidden_layer_output * (1 - hidden_layer_output))\n",
    "    differentiated_input = np.where(hidden_layer_input > 0, 1, 0) # ReLUに入力するhidden_input_layerの微分\n",
    "    dEn_dX_sig = dEn_dX * differentiated_input\n",
    "    \n",
    "    dEn_dW_2= np.dot(dEn_dX_sig.T, batch_image_vector)\n",
    "    dEn_db_2 = np.sum(dEn_dX_sig, axis=0)\n",
    "\n",
    "    # --- パラメータ更新 ---\n",
    "    weight1 -= dEn_dW_2 * learning_rate \n",
    "    bias1   -= dEn_db_2 * learning_rate\n",
    "    weight2 -= dEn_dW_1 * learning_rate \n",
    "    bias2   -= dEn_db_1 * learning_rate\n",
    "    \n",
    "    return weight1, bias1, weight2, bias2\n",
    "\n",
    "def backward_propagation_and_update_train(batch_image_vector, hidden_layer_input, hidden_layer_output, output_probabilities, one_hot_labels, \n",
    "                                    weight1, bias1, weight2, bias2, learning_rate, ignore_number, momentum, prev_delta_W1, prev_delta_W2):\n",
    "    current_batch_size = batch_image_vector.shape[0]\n",
    "    dEn_dak = (output_probabilities - one_hot_labels) / current_batch_size\n",
    "    dEn_dX = np.dot(dEn_dak, weight2)\n",
    "    dEn_dW_1 = np.dot(dEn_dak.T, hidden_layer_output)\n",
    "    dEn_db_1 = np.sum(dEn_dak, axis = 0)\n",
    "    # dEn_dX_sig = dEn_dX * (hidden_layer_output * (1 - hidden_layer_output))\n",
    "    differentiated_input = np.where(hidden_layer_input > 0, 1, 0) # ReLUに入力するhidden_input_layerの微分\n",
    "    for index in ignore_number:\n",
    "        dEn_dX[:, index] = 0 \n",
    "        differentiated_input[:, index] = 0 \n",
    "    dEn_dX_sig = dEn_dX * differentiated_input \n",
    "    dEn_dW_2= np.dot(dEn_dX_sig.T, batch_image_vector)\n",
    "    dEn_db_2 = np.sum(dEn_dX_sig, axis=0)\n",
    "    weight1 = weight1 + (momentum * prev_delta_W1 - dEn_dW_2 * learning_rate )\n",
    "    bias1   -= dEn_db_2 * learning_rate\n",
    "    weight2 = weight2 + (momentum * prev_delta_W2 - dEn_dW_1 * learning_rate )\n",
    "    bias2   -= dEn_db_1 * learning_rate\n",
    "    \n",
    "    params_delta_W1 = dEn_dW_2 * learning_rate\n",
    "    params_delta_W2 = dEn_dW_1 * learning_rate\n",
    "    return weight1, bias1, weight2, bias2, params_delta_W1, params_delta_W2\n",
    "\n",
    "def get_accuracy(y_prop, y_true): # 正答率計算\n",
    "\n",
    "    y_pred = get_predicted_class(y_prop) # 予測結果\n",
    "\n",
    "    accuracy = np.sum(y_pred == y_true) / len(y_prop)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def calculate_accuracy_for_epoch(images, labels, weight1, bias1, weight2, bias2, mode, ignore_number):\n",
    "    \"\"\"\n",
    "    指定されたデータセットに対するモデルの正答率を計算する関数。\n",
    "    modeに応じてforward_propagation_train/testを呼び出す。\n",
    "    \"\"\"\n",
    "    # images_vector = images.reshape(len(images), -1)\n",
    "\n",
    "    if mode == 'train':\n",
    "\n",
    "        probabilities, _, _ = forward_propagation_train(images, weight1, bias1, weight2, bias2, ignore_number)\n",
    "    elif mode == 'test':\n",
    "        probabilities, _, _ = forward_propagation_test(images, weight1, bias1, weight2, bias2, ignore_number)\n",
    "    else:\n",
    "         # デフォルト\n",
    "        probabilities, _, _ = forward_propagation(images, weight1, bias1, weight2, bias2)\n",
    "\n",
    "    accuracy = get_accuracy(probabilities, labels)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# --- メイン処理 ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    batch_size = 100\n",
    "    epoch_number = 10\n",
    "    learning_rate = 0.01\n",
    "    train_loss_list, train_acc_list, test_acc_list = [], [], []\n",
    "    momentum = 0.9\n",
    "    prev_delta_W1 = 0\n",
    "    prev_delta_W2 = 0\n",
    "\n",
    "    while True:\n",
    "        mode = str(input('実行モードを入力してください (train or test): '))\n",
    "        if mode in ['train', 'test']:\n",
    "            break\n",
    "        print(\"無効なモードです。'train' または 'test' を入力してください。\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            ignore_number = int(input(f'Dropoutの個数を 0 ~ {hidden_layer_size} で入力してください: '))\n",
    "            if 0 <= ignore_number <= hidden_layer_size:\n",
    "                break\n",
    "            else:\n",
    "                print(f\"無効なドロップアウト数です。0から{hidden_layer_size}の範囲で入力してください。\")\n",
    "        except ValueError:\n",
    "            print(\"無効な入力です。整数を入力してください。\")\n",
    "\n",
    "    # 訓練モードの場合にのみ学習を実行\n",
    "    if mode == 'train':\n",
    "        print(\"\\n--- 訓練モード実行中 ---\")\n",
    "\n",
    "        for i in range(1, epoch_number + 1):\n",
    "            error_sum = 0\n",
    "            train_accuracy_sum = 0\n",
    "            shuffled_train_image_index = get_shuffled_index(train_images)\n",
    "            \n",
    "            for j in range(0, len(shuffled_train_image_index), batch_size): # range(start, stop, step) を使い、batch_sizeずつインデックスをずらしながらループ\n",
    "\n",
    "                # hidden_layer_size分のインデックス配列からignore_number個ランダムに選択\n",
    "                random_selection = np.random.choice(np.arange(hidden_layer_size), size=ignore_number, replace=False)\n",
    "                \n",
    "                index = shuffled_train_image_index[j:j + batch_size]    #シャッフルしたインデックスから、先頭のbatch_size分取り出す\n",
    "\n",
    "                # 統合した関数を使い、ミニバッチと対応ラベルを一度に取得\n",
    "                # ミニバッチ取得（flattened を受け取る）\n",
    "                batch_image, batch_labels = get_batch(index)\n",
    "                \n",
    "                # 全結合向け入力をそのまま使用（im2col は使わない）\n",
    "                train_images_col = batch_image  # shape: (B, 3072)\n",
    "                \n",
    "                # 順伝播を実行\n",
    "                output_probabilities, hidden_layer_input, hidden_layer_output = forward_propagation_train(\n",
    "                    train_images_col, weight1, bias1, weight2, bias2, random_selection\n",
    "                )\n",
    "                # one-hot labelsを取得\n",
    "                one_hot_labels = get_one_hot_label(batch_labels, output_layer_size)\n",
    "\n",
    "                \n",
    "                # クロスエントロピー誤差平均を計算\n",
    "                calculated_error = get_cross_entropy_error(output_probabilities, one_hot_labels)\n",
    "                error_sum += calculated_error\n",
    "                # --- 逆伝播 ---\n",
    "                weight1, bias1, weight2, bias2, prev_delta_W1, prev_delta_W2 = backward_propagation_and_update_train(\n",
    "                    batch_image, hidden_layer_input, hidden_layer_output, output_probabilities, one_hot_labels,\n",
    "                    weight1, bias1, weight2, bias2, learning_rate, random_selection, momentum, prev_delta_W1, prev_delta_W2\n",
    "                )\n",
    "                train_accuracy_sum += calculate_accuracy_for_epoch(train_images_col, batch_labels, weight1, bias1, weight2, bias2, 'train', random_selection)\n",
    "\n",
    "            ignore_index_for_acc = np.arange(hidden_layer_size)[:ignore_number] \n",
    "            \n",
    "             # テスト用も平坦化して渡す（im2col を使わない）\n",
    "            test_images_col = padded_test_images.reshape(len(padded_test_images), -1)\n",
    "            test_accuracy = calculate_accuracy_for_epoch(test_images_col, test_labels, weight1, bias1, weight2, bias2, 'test', ignore_index_for_acc)\n",
    "# ...existing code...\n",
    "            num_batches = len(train_images) // batch_size\n",
    "            train_loss_list.append(error_sum / num_batches)\n",
    "            train_acc_list.append(train_accuracy_sum/ num_batches)\n",
    "            test_acc_list.append(test_accuracy)\n",
    "            print(f\"{i}エポック目\")\n",
    "            print(f\"  平均クロスエントロピー誤差: {error_sum / num_batches}\")\n",
    "            print(f\"  学習データに対する正答率: {train_accuracy_sum / num_batches}\")\n",
    "            print(f\"  テストデータに対する正答率: {test_accuracy}\")\n",
    "        \n",
    "    # --- グラフの描画 ---\n",
    "        x = np.arange(1, epoch_number + 1)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # 誤差のグラフ\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(x, train_loss_list, marker='o')\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "\n",
    "        # 正答率のグラフ\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(x, train_acc_list, marker='o', label='Train Accuracy')\n",
    "        plt.plot(x, test_acc_list, marker='s', label='Test Accuracy')\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        np.savez('assignment3_parameter.npz', weight1 = weight1, bias1 = bias1, weight2 = weight2, bias2 = bias2)\n",
    "    # テストモードの場合にのみ予測を実行\n",
    "    elif mode == 'test':\n",
    "        print(\"\\n--- テストモード実行中 ---\")\n",
    "        random_selection = np.random.choice(np.arange(hidden_layer_size), size=ignore_number, replace=False)\n",
    "        \n",
    "        # パディングを除去してから平坦化\n",
    "        test_images_no_pad = padded_test_images[:, 1:-1, 1:-1, :]\n",
    "        test_images_flat = test_images_no_pad.reshape(len(test_images_no_pad), -1)\n",
    "        \n",
    "        # テストデータに対する最終的な正答率を計算\n",
    "        test_accuracy = calculate_accuracy_for_epoch(\n",
    "            test_images_flat, test_labels, weight1, bias1, weight2, bias2, 'test', random_selection\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nテストデータに対する最終正答率: {test_accuracy}\")\n",
    "        print(f\"（ドロップアウト数 {ignore_number} 個）\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
