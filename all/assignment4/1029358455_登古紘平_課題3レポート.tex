\documentclass[11px,a4,dvipdfmx]{jsarticle}
% 数式
\usepackage{amsmath,amsfonts}
\usepackage{bm}
\usepackage{siunitx}
\usepackage{here}
% 画像
\usepackage[dvipdfmx]{graphicx}
\usepackage[dvipdfmx]{color}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}
%ソースコード
\usepackage{listings} % jlisting を削除
\lstset{
    language=Verilog,
    basicstyle={\ttfamily},
    identifierstyle={\small},
    commentstyle={\smallitshape},
    keywordstyle={\small\bfseries},
    ndkeywordstyle={\small},
    stringstyle={\small\ttfamily},
    frame={tb},
    breaklines=true,
    columns=[l]{fullflexible},
    numbers=left,
    xrightmargin=0zw,
    xleftmargin=3zw,
    numberstyle={\scriptsize},
    stepnumber=1,
    numbersep=1zw,
    lineskip=-0.5ex,
    language=Verilog,
}
\renewcommand{\lstlistingname}{コード}

\begin{document}

\title{計算機科学実験及演習4 画像処理 レポート}
\author{工学部 情報学科 計算機科学コース\\学生番号: 1029358455 \hspace{1em} 氏名: 登古紘平}
\date{\today}
\maketitle
\newpage
\section{4-1}
\begin{center}
\textbf{課題内容}
\end{center}
活性化関数としてシグモイド関数の他に次式で挙げる \textbf{ReLU} もよく用いられる．\\
$$
a(t) = 
\begin{cases}
t & (t > 0) \\
0 & (t \le 0)
\end{cases}
$$

\vspace{5mm}

ReLU の微分は，\\
$$
a(t)' = 
\begin{cases}
1 & (t > 0) \\
0 & (t \le 0)
\end{cases}\\
$$
で与えられる．

\subsection{作成したプログラムの説明}
このプログラムでは、活性化関数として、シグモイド関数の代わりに ReLU を用いる。これは、入力の値によって出力の仕方が変化するので、中間層に対する入力である hidden\_layer\_input を順伝播実行時に出力させ、それを逆伝播及び ReLU の微分に用いた。
今回作成したReLU関数は以下の通りである。\\\\
def ReLU(arr):\\
    new\_arr = np.where(arr $>$ 0, arr, 0)\\
    return new\_arr\\\\
また、逆伝播における微分を以下のように表した。 \\\\
    \# dEn\_dX\_sig = dEn\_dX * (hidden\_layer\_output * (1 - hidden\_layer\_output))\\
    differentiated\_input = np.where(hidden\_layer\_input $>$ 0, 1, 0) \# ReLUに入力するhidden\_input\_layerの微分\\
    dEn\_dX\_sig = dEn\_dX * differentiated\_input
\subsection{実行結果}
\begin{verbatim}
ロードしますか? yes or no: no

1エポック目
  平均クロスエントロピー誤差: 1.794267525319996
  テストデータに対する正答率: 0.5322
  学習データに対する正答率: 0.426833333333334
2エポック目
  平均クロスエントロピー誤差: 1.385299717909186
  テストデータに対する正答率: 0.5804
  学習データに対する正答率: 0.5111833333333332
  ...
10エポック目
  平均クロスエントロピー誤差: 0.7004218257765509
  テストデータに対する正答率: 0.8188
  学習データに対する正答率: 0.7939166666666663
\end{verbatim}
 \begin{figure}[H]
		      \begin{center}
			      \includegraphics[scale=0.3]{picture/4-1-noLoaded.png}
			      \caption{4-1 プロット結果(ロード無し)}
		      \end{center}
\end{figure}
\begin{verbatim}
ロードしますか? yes or no: yes

1エポック目
  平均クロスエントロピー誤差: 0.8011985461454941
  テストデータに対する正答率: 0.7324
  学習データに対する正答率: 0.7313666666666673
2エポック目
  平均クロスエントロピー誤差: 0.7101978870361783
  テストデータに対する正答率: 0.7315
  学習データに対する正答率: 0.794383333333333
  ...
10エポック目
  平均クロスエントロピー誤差: 0.9641420967176714
  テストデータに対する正答率: 0.6221
  学習データに対する正答率: 0.6602499999999994
\end{verbatim}
 \begin{figure}[H]
		      \begin{center}
			      \includegraphics[scale=0.3]{picture/4-1-loeded.png}
			      \caption{4-1 プロット結果(ロード有り)}
		      \end{center}
\end{figure}
\subsection{工夫点}
中間層の活性化関数をシグモイド関数から ReLU に変更するにあたり、逆伝播時の勾配計算に必要な情報を正確に伝達する工夫を施した。具体的には、ReLU の微分が $t>0$ で 1、 $t \le 0$ で 0 となる性質を利用するため、順伝播の際に ReLU への入力値 (\texttt{hidden\_layer\_input}) を保持し、戻り値として追加した。
これにより、逆伝播において \texttt{hidden\_layer\_input} を基に、
ReLU の正しい勾配を計算し、元のコードの主要な構造を維持したまま機能変更を達成することができた。

\subsection{問題点と考察}
プログラムの動作自体は確認できたが、実行結果には学習の効率と安定性に関して複数の問題点が観察された。

\begin{itemize}
	\item \textbf{正答率の低下}: パラメータのロードの有無にかかわらず、ReLU 実装後の正答率が以前のシグモイド実装時と比較して低下する傾向が見られた。
	\item \textbf{学習の不安定化（ロード有り）}: 既存のパラメータをロードして学習を再開した場合、平均クロスエントロピー誤差と正答率がエポック間で激しく上下し、結果が不安定になった。
\end{itemize}
これらの問題を踏まえ、今後は、学習の安定化のために学習率を調整するなどの工夫が必要であると感じた。

\subsection{リファクタリング}
リファクタリングには、Gemini(2.5 Flash)を用いた。
\begin{itemize}
    \item プロンプト: (コード、プロット結果貼り付け) これは、３層ニューラルネットワークにおいて、Dropoutを実装したものである。これに論理エラーはあるか
    \item 目的: 認識できていない論理エラーの検出、ReLU関数への理解向上のため
\end{itemize}
結論として、ReLU 関数はうまく実装できており、正答率や平均クロスエントロピー誤差が不安定な理由として4つの理由が挙げられた。
\begin{itemize}
    \item 学習率が大きすぎる可能性
    \item 重みの初期化が不適切（過度に大きい、小さいなど）
    \item バッチサイズとエポック数の影響
    \item 汎化性能の問題
\end{itemize}
これを踏まえ、学習したパラメータの初期化、学習率を0.005に、エポック数を25に変更した。その結果が以下である。
 \begin{figure}[H]
		      \begin{center}
			      \includegraphics[scale=0.3]{picture/4-1-loaded-modified.png}
			      \caption{4-1 プロット結果(ロード有り、修正後)}
		      \end{center}
\end{figure}
以上より、正答率、平均クロスエントロピー誤差ともに安定したことがわかる。修正前後で比較すると、1エポック目の値に注目すると、ロードしていたパラメータに問題があったのではないかと考えられる。

\section{4-2}
\begin{center}
\textbf{課題内容}
\end{center}
Dropout は学習時に中間層のノードをランダムに選び，その出力を無視（出力 $= 0$）して学習する手法である．無視するノードの選択は学習データ毎にランダムに行い，中間層全体のノード数 $\times \rho$ 個のノードの出力を無視する．

テスト時は，全てのノードの出力を無視せず，代わりに元の出力に $(1 - \rho)$ 倍したものを出力として用いる．このように，Dropout は学習時とテスト時で振る舞いが異なるので，学習時かテスト時かを判定するフラグを用意しておく必要がある．

Dropout を活性化関数の一種と考えると，
\begin{equation}
a(t) = 
\begin{cases}
t & (\text{ノードが無視されない場合}) \\
0 & (\text{ノードが無視された場合})
\end{cases}
\end{equation}
となり，Dropout の微分は，
\begin{equation}
a(t)' = 
\begin{cases}
1 & (\text{ノードが無視されない場合}) \\
0 & (\text{ノードが無視された場合})
\end{cases}
\end{equation}
で与えられる．
\subsection{作成したプログラムの説明}
今回、訓練時とテスト時でプログラムの振る舞いが異なる。そのため、訓練時かテスト時かどうかを標準有力として受け取り、それをmodeという変数に代入し、その値に応じて条件分岐させた。
\begin{verbatim}
    mode = str(input('実行モードを入力してください (train or test): '))
    if mode not in ['train', 'test']:
        print("無効なモードです。'train' または 'test' を入力してください。")
        exit()

    ignore_number = int(input('Dropoutの個数を 0 ~ 100 で入力してください: '))
    if not (0 <= ignore_number <= hidden_layer_size):
        print("無効なドロップアウト数です。0から100の範囲で入力してください。")
        exit()

    # 訓練モードの場合にのみ学習を実行
    if mode == 'train':
        print("\n--- 訓練モード実行中 ---")
    ...
    
        # テストモードの場合にのみ予測を実行
    elif mode == 'test':
        print("\n--- テストモード実行中 ---")
        random_selection = np.random.choice(np.arange(hidden_layer_size), size=ignore_number, replace=False)
    # テストデータに対する最終的な正答率を計算
        test_accuracy = calculate_accuracy_for_epoch(test_images, test_labels, weight1, bias1, weight2, bias2, 'test', random_selection)

        print(f"\nテストデータに対する最終正答率: {test_accuracy}")
        print(f"（ドロップアウト数 {ignore_number} 個）")
\end{verbatim}
また、今回新たに、Dropoutを考慮した、訓練時、テスト時の順伝播、訓練時の逆伝播の3つの関数を、既存のコードをベースに作成した。
\begin{verbatim}
def forward_propagation_train(input_vector, weight1, bias1, weight2, bias2, ignore_number):
    hidden_layer_input = np.dot(input_vector, weight1.T) + bias1
    hidden_layer_output = ReLU(hidden_layer_input)
    for index in ignore_number:
        hidden_layer_output[:, index] = 0 # 無視する値を０に
    output_layer_input = np.dot(hidden_layer_output, weight2.T) + bias2
    final_output = softmax(output_layer_input)
    return final_output, hidden_layer_input, hidden_layer_output  

def forward_propagation_test(input_vector, weight1, bias1, weight2, bias2, ignore_number):
    
    ...
    hidden_layer_output *= 1 - (len(ignore_number) / hidden_layer_size)
    ...

def backward_propagation_and_update_train(batch_image_vector, hidden_layer_input, 
    hidden_layer_output, output_probabilities, one_hot_labels,  weight1, bias1, weight2, 
    bias2, learning_rate, ignore_number):

    ...
    # dEn_dX_sig = dEn_dX * (hidden_layer_output * (1 - hidden_layer_output))
    differentiated_input = np.where(hidden_layer_input > 0, 1, 0) # ReLUに入力するhidden_input_layerの微分
    for index in ignore_number:
        dEn_dX[:, index] = 0 
        differentiated_input[:, index] = 0 
    dEn_dX_sig = dEn_dX * differentiated_input 
    ...
\end{verbatim}
以上の通り、順伝播において、訓練時は、ランダムに取得したインデックス配列をfor文で回し、各要素を０にしており、テスト時は、各要素の値を $(1 - \rho)$倍にしている。逆伝播においても同様である。
\subsection{実行結果}
\begin{verbatim}
ロードしますか? yes or no: no
実行モードを入力してください (train or test): train
Dropoutの個数を 0 ~ 100 で入力してください: 20

--- 訓練モード実行中 ---
1エポック目
  平均クロスエントロピー誤差: 1.9865031046553099
  学習データに対する正答率: 0.3299000000000002
  テストデータに対する正答率: 0.4549
2エポック目
  平均クロスエントロピー誤差: 1.5910906225606625
  学習データに対する正答率: 0.4241833333333337
  テストデータに対する正答率: 0.5275
...
10エポック目
  平均クロスエントロピー誤差: 1.50288702684242
  学習データに対する正答率: 0.42561666666666603
  テストデータに対する正答率: 0.4531
\end{verbatim}
 \begin{figure}[H]
		      \begin{center}
			      \includegraphics[scale=0.3]{picture/4-2-noloaded-20.png}
			      \caption{4-2 プロット結果(ロード無し、Dropout = 20)}
		      \end{center}
\end{figure}
\begin{verbatim}
ロードしますか? yes or no: yes
実行モードを入力してください (train or test): train
Dropoutの個数を 0 ~ 100 で入力してください: 20

--- 訓練モード実行中 ---
1エポック目
  平均クロスエントロピー誤差: 1.4786787110387787
  学習データに対する正答率: 0.4321666666666662
  テストデータに対する正答率: 0.4589
2エポック目
  平均クロスエントロピー誤差: 1.4163873100601898
  学習データに対する正答率: 0.47068333333333345
  テストデータに対する正答率: 0.5262
...
10エポック目
  平均クロスエントロピー誤差: 1.4476414952641528
  学習データに対する正答率: 0.4366666666666658
  テストデータに対する正答率: 0.4498
\end{verbatim}
 \begin{figure}[H]
		      \begin{center}
			      \includegraphics[scale=0.3]{picture/4-2-loaded-20.png}
			      \caption{4-2 プロット結果(ロード有り、Dropout = 20)}
		      \end{center}
\end{figure}

\begin{verbatim}
ロードしますか? yes or no: no
実行モードを入力してください (train or test): test
Dropoutの個数を 0 ~ 100 で入力してください: 10

--- テストモード実行中 ---

テストデータに対する最終正答率: 0.1062
（ドロップアウト数 10 個）

ロードしますか? yes or no: no
実行モードを入力してください (train or test): test
Dropoutの個数を 0 ~ 100 で入力してください: 80

--- テストモード実行中 ---

テストデータに対する最終正答率: 0.1057
（ドロップアウト数 80 個）

ロードしますか? yes or no: yes
実行モードを入力してください (train or test): test
Dropoutの個数を 0 ~ 100 で入力してください: 10

--- テストモード実行中 ---

テストデータに対する最終正答率: 0.4505
（ドロップアウト数 10 個）
\end{verbatim}
\subsection{工夫点}
訓練時かテスト時かどうか判定するフラグを標準入力で受け取り、その文字列を関数が受け取ることで、内部で用いる関数を変化させた。これにより、それぞれの場合での関数実行の流れが明確になっただけでなく、直接文字列を指定して、正答率計算を簡潔に記述することができた。
\subsection{問題点と考察}
正答率計算においては、工夫点として上にあげたように簡潔に条件分岐を記述することができたが、正答率計算関数の内部で用いた順伝播関数は、訓練時、テスト時それぞれ分けて記述されており、記述量が多くなり冗長なコードとなってしまった。また、Dropoutにおいて、出力を無視する要素を選択するメソッドが少しわかりにくくなってしまった。また、条件分岐の際にexit()を用いたため、カーネルがクラッシュしてしまった。
\subsection{リファクタリング}
リファクタリングには、Gemini(2.5 Flash)を用いた。
\begin{itemize}
    \item プロンプト: (コード、プロット結果貼り付け)  これは、３層ニューラルネットワークにおいて、Dropoutを実装したものである。これに論理エラーはあるか。
    \item 目的: 認識できていない論理エラーの検出
\end{itemize}
結果、現在のコードはスケーリングが施されておらず、期待値計算として好ましくないと回答された。ドロップアウトにおいてスケーリング（$\frac{1}{1-p}$ や $1-p$ を乗じる処理）を行う目的は、ニューロンの出力の期待値を維持することであり、これは、訓練時とテスト時の間で、ニューラルネットワークの出力量のスケールが急激に変化するのを防ぎ、予測性能を安定させるために不可欠な処理であるという。今回の課題では、スケーリングの実装は要求されていないため、リファクタリングの結果として、カーネルのクラッシュを改善した。
\begin{itemize}
    \item プロンプト: (コード貼り付け)  これは、３層ニューラルネットワークにおいて、Dropoutを実装したものである。不正な値が入力された際にカーネルがクラッシュしてしまう。原因と解決策を教えて
    \item 目的: カーネルがクラッシュしてしまう原因の究明とその改善
\end{itemize}
\begin{verbatim}
リファクタリング前

    mode = str(input('実行モードを入力してください (train or test): '))
    if mode not in ['train', 'test']:
        print("無効なモードです。'train' または 'test' を入力してください。")
        exit()

    ignore_number = int(input('Dropoutの個数を 0 ~ 100 で入力してください: '))
    if not (0 <= ignore_number <= hidden_layer_size):
        print("無効なドロップアウト数です。0から100の範囲で入力してください。")
        exit()

リファクタリング後

    while True:
        mode = str(input('実行モードを入力してください (train or test): '))
        if mode in ['train', 'test']:
            break
        print("無効なモードです。'train' または 'test' を入力してください。")

    while True:
        try:
            ignore_number = int(input(f'Dropoutの個数を 0 ~ {hidden_layer_size} で入力してください: '))
            if 0 <= ignore_number <= hidden_layer_size:
                break
            else:
                print(f"無効なドロップアウト数です。0から{hidden_layer_size}の範囲で入力してください。")
        except ValueError:
            print("無効な入力です。整数を入力してください。")
\end{verbatim}
exit()を用いずに、whileでループさせることによって、カーネルのクラッシュを回避した。また、Dropoutの個数の指定を、"0から100"としていたが、中間層の個数が変化した場合にも対応するため、"0からhidden\_layer\_size"とした。

\section{4-4 慣性項}
\begin{center}
\textbf{課題内容}
\end{center}
\textbf{慣性項(Momentum)付き SGD} 慣性項(Momentum)付き SGDでは，パラメータ $W$ の更新量に前回の更新量の $\alpha$ 倍を加算する手法である．事前に設定する必要のあるパラメータは学習率 $\eta$ と $\alpha$ である．($\eta=0.01$, $\alpha=0.9$ くらいがおすすめ)
$$
\Delta W \leftarrow \alpha \Delta W - \eta \frac{\partial E_n}{\partial W} \tag{41}
$$
$$
W \leftarrow W + \Delta W \tag{42}
$$
なお，$\Delta W$ の初期値は $\mathbf{0}$ とする．
\subsection{作成したプログラム}
この課題では、訓練時、逆伝播において重みを更新する backward\_propagation\_and\_update\_train のプロセスに変更を加えた。以下がそのコードの差分である。
\begin{verbatim}
リファクタリング前

def backward_propagation_and_update_train(..., ignore_number):

    ...
    
    weight1 -= dEn_dW_2 * learning_rate 
    bias1   -= dEn_db_2 * learning_rate
    weight2 -= dEn_dW_1 * learning_rate 
    bias2   -= dEn_db_1 * learning_rate
    
    return weight1, bias1, weight2, bias2
    
リファクタリング後

def backward_propagation_and_update_train(..., ignore_number, momentum, prev_delta_W1, prev_delta_W2):

    ...
    
    weight1 = weight1 + (momentum * prev_delta_W1 - dEn_dW_2 * learning_rate )
    bias1   -= dEn_db_2 * learning_rate
    weight2 = weight2 + (momentum * prev_delta_W2 - dEn_dW_1 * learning_rate )
    bias2   -= dEn_db_1 * learning_rate
    
    params_delta_W1 = dEn_dW_2 * learning_rate
    params_delta_W2 = dEn_dW_1 * learning_rate
    
    return weight1, bias1, weight2, bias2, params_delta_W1, params_delta_W2
\end{verbatim}
新たに慣性項パラメータ $\alpha$ ＝ 0.9 、前回の重みの変化量である params\_delta\_W1, params\_delta\_W2 を導入し、資料に従って重みの更新を行った。重みの変化量を、
\begin{verbatim}
    params_delta_W1 = dEn_dW_2 * learning_rate
    params_delta_W2 = dEn_dW_1 * learning_rate
\end{verbatim}
とするか、
\begin{verbatim}
    params_delta_W1 = momentum * prev_delta_W1 - dEn_dW_2 * learning_rate
    params_delta_W2 = momentum * prev_delta_W2 - dEn_dW_1 * learning_rate
\end{verbatim}
とするか２通りの実装の方法が考えられたが、今回は前者の形をとっている。また、メイン処理部分において、params\_delta\_W1, params\_delta\_W2 = 0 の初期化を行っている。
\subsection{実行結果}
\begin{verbatim}
ロードしますか? yes or no: no
実行モードを入力してください (train or test): train
Dropoutの個数を 0 ~ 100 で入力してください: 10

--- 訓練モード実行中 ---
1エポック目
  平均クロスエントロピー誤差: 2.384880129041223
  学習データに対する正答率: 0.2312666666666665
  テストデータに対する正答率: 0.3338
2エポック目
  平均クロスエントロピー誤差: 1.7131512353370284
  学習データに対する正答率: 0.3887166666666666
  テストデータに対する正答率: 0.4506
  ...
10エポック目
  平均クロスエントロピー誤差: 0.8172133287892329
  学習データに対する正答率: 0.7791333333333331
  テストデータに対する正答率: 0.7523
\end{verbatim}
 \begin{figure}[H]
		      \begin{center}
			      \includegraphics[scale=0.3]{picture/4-4-momentum-noLoaded-10.png}
			      \caption{4-4(慣性項) プロット結果(ロードなし、Dropout = 10)}
		      \end{center}
\end{figure}
\begin{verbatim}
ロードしますか? yes or no: no
実行モードを入力してください (train or test): train
Dropoutの個数を 0 ~ 100 で入力してください: 30

--- 訓練モード実行中 ---
1エポック目
  平均クロスエントロピー誤差: 2.19495728323395
  学習データに対する正答率: 0.3730166666666668
  テストデータに対する正答率: 0.559
2エポック目
  平均クロスエントロピー誤差: 1.45440449015992
  学習データに対する正答率: 0.530683333333333
  テストデータに対する正答率: 0.6373
  ...
10エポック目
  平均クロスエントロピー誤差: 0.7935305138366348
  学習データに対する正答率: 0.7825833333333335
  テストデータに対する正答率: 0.8966
\end{verbatim}
 \begin{figure}[H]
		      \begin{center}
			      \includegraphics[scale=0.3]{picture/4-4-momentum-noLoaded-30.png}
			      \caption{4-4(慣性項) プロット結果(ロードなし、Dropout = 30)}
		      \end{center}
\end{figure}
プロット結果を見ると、今まで不安定だったグラフが、着実に精度を上げ、大幅に完全されたことがわかる。また、Dropoutの数によって、正答率や、訓練データとテストデータの差が大きく変化することもわかる。ただ、この差は、前回の課題で述べたスケーリングを実装すれば減少するのではないかと考えた。
\subsection{リファクタリング}
リファクタリングには、Gemini(2.5 Flash)を用いた。
\begin{itemize}
    \item プロンプト: (コード貼り付け)  これは、３層ニューラルネットワークにおいて、慣性項を実装したものである。これは慣性項を正しく実装できているか、また、慣性項を実装する利点を教えて
    \item 目的: 論理エラーの改善、慣性項に対する理解向上
\end{itemize}
この結果、慣性項の実装がうまく行えていることが分かった。また、慣性項には以下の利点があることが分かった。
\begin{itemize}
    \item 学習の高速化: Momentumは、更新ベクトル（$\Delta W$）に「慣性」を持たせることで、同じ方向の勾配が続く場合、更新が加速される。これにより、最適解に素早く到達する。
    \item 最適解への収束性の向上: Momentumは、振動方向の勾配を相殺し、一貫性のある谷底への方向に加速するため、効率的な収束を助ける。
\end{itemize}
\end{document}