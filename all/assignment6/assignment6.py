import os
from pathlib import Path
import urllib.request
import tarfile
import pickle
import numpy as np
import matplotlib.pyplot as plt

POOL_SIZE = 2
POOL_STRIDE = 2

def ensure_cifar(data_dir="all/cifar-10-batches-py"):
    data_dir = Path(data_dir)
    # æ—¢ã«æŒ‡å®šãƒ‘ã‚¹ã«å­˜åœ¨ã™ã‚Œã°è¿”ã™
    if (data_dir / "data_batch_1").exists():
        print("CIFAR-10 ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™:", data_dir)
        return str(data_dir)

    # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹å†…ã‚’æ¢ç´¢ã—ã¦è¦‹ã¤ã‹ã‚Œã°è¿”ã™
    ws_root = Path(r"c:\Users\tokot\code\image-processing")
    for p in ws_root.rglob("data_batch_1"):
        print("ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹å†…ã§ç™ºè¦‹:", p.parent)
        return str(p.parent)

    # è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å±•é–‹
    print("CIFAR-10 ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™...")
    dest = ws_root / "cifar-10-python.tar.gz"
    url = "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
    try:
        urllib.request.urlretrieve(url, dest)
        print("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†:", dest)
        with tarfile.open(dest, "r:gz") as tf:
            tf.extractall(path=ws_root)
        extracted = ws_root / "cifar-10-batches-py"
        if (extracted / "data_batch_1").exists():
            print("å±•é–‹å®Œäº†:", extracted)
            return str(extracted)
        raise FileNotFoundError("å±•é–‹å¾Œã«æœŸå¾…ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    except Exception as e:
        print("å–å¾—å¤±æ•—:", e)
        raise

def load_cifar10(data_dir):
    """ (N,3072) ã® train/test ã¨ãƒ©ãƒ™ãƒ«ã‚’è¿”ã™ï¼ˆfloat32 0-1 æ­£è¦åŒ–ï¼‰ """
    train_data = None
    train_labels = []
    data_dir = str(data_dir)
    for i in range(1, 6):
        path = os.path.join(data_dir, f"data_batch_{i}")
        if not os.path.exists(path):
            raise FileNotFoundError(path)
        with open(path, "rb") as f:
            data_dict = pickle.load(f, encoding="bytes")
            if train_data is None:
                train_data = data_dict[b"data"]
            else:
                train_data = np.vstack((train_data, data_dict[b"data"]))
            train_labels.extend(data_dict[b"labels"])
    test_path = os.path.join(data_dir, "test_batch")
    with open(test_path, "rb") as f:
        data_dict = pickle.load(f, encoding="bytes")
        test_data = data_dict[b"data"]
        test_labels = data_dict[b"labels"]

    train_data = np.array(train_data, dtype=np.float32) / 255.0
    test_data = np.array(test_data, dtype=np.float32) / 255.0
    train_labels = np.array(train_labels, dtype=np.int64)
    test_labels = np.array(test_labels, dtype=np.int64)
    return train_data, train_labels, test_data, test_labels


def padding_data(train_images, test_images, pad=1):
    # (N, 3072) -> (N, 3, 32, 32) -> (N, 32, 32, 3)
    train_imgs = train_images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)
    test_imgs = test_images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)

    padded_train = np.pad(
        train_imgs,
        ((0, 0), (pad, pad), (pad, pad), (0, 0)),
        mode="constant",
        constant_values=0.0,
    )
    padded_test = np.pad(
        test_imgs,
        ((0, 0), (pad, pad), (pad, pad), (0, 0)),
        mode="constant",
        constant_values=0.0,
    )

    return padded_train, padded_test


def im2col(padding_data, filter_size, stride=1, pad=0):
    N, H, W, C = padding_data.shape
    
    # å‡ºåŠ›ç‰¹å¾´ãƒãƒƒãƒ—ã®ã‚µã‚¤ã‚ºè¨ˆç®—
    out_h = (H - filter_size) // stride + 1
    out_w = (W - filter_size) // stride + 1
    
    col = np.zeros((N, out_h, out_w, filter_size, filter_size, C))

    for i in range(out_h):
        i_max = i * stride + filter_size
        for j in range(out_w):
            j_max = j * stride + filter_size
            
            # ãƒ‘ãƒƒãƒã‚’æŠ½å‡º
            col[:, i, j, :, :, :] = padding_data[:, i * stride:i_max, j * stride:j_max, :]
            
    # å½¢çŠ¶ã‚’ (R*R*C, N*out_h*out_w) ã«å¤‰æ›ï¼ˆè¡Œåˆ—ä¹—ç®—ã®å½¢å¼ï¼‰
    col = col.reshape(-1, filter_size * filter_size * C).T
    
    return col

def set_filter_weights():
    K = 32  # ãƒ•ã‚£ãƒ«ã‚¿æšæ•°
    R = 3   # ãƒ•ã‚£ãƒ«ã‚¿ã‚µã‚¤ã‚º
    ch = 3  # å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°
    input_node_count = R * R * ch 
    std_dev = np.sqrt(2.0 / input_node_count)
    # å½¢çŠ¶ã¯ (K, R*R*ch) = (32, 27)
    W = std_dev * np.random.randn(K, R * R * ch)
    
    return W, R

def set_biases():
    K = 32  # ãƒ•ã‚£ãƒ«ã‚¿æšæ•°
    b = np.random.normal(loc=0.0, scale=0.01, size=K)
    b_vector = b.reshape(-1, 1)
    
    return b_vector

def get_shuffled_index(arr):
    index_arr = np.arange(len(arr)) 
    np.random.shuffle(index_arr) 
    return index_arr

def get_batch(random_index): 
    # ãƒ™ã‚¯ãƒˆãƒ«ã¨ãƒ©ãƒ™ãƒ«ã‚’ã¾ã¨ã‚ã¦å–å¾—
    batch_images = padded_train_images[random_index]
    batch_labels = train_labels[random_index]
    return batch_images, batch_labels

def get_one_hot_label(batch_labels, output_layer_size):
    one_hot_labels = np.zeros((batch_labels.size, output_layer_size)) 
    one_hot_labels[np.arange(batch_labels.size), batch_labels] = 1 
    return one_hot_labels

def conv_forward(padded_data, conv_W, conv_b_vector, filter_size, stride=1):
    # ç•³ã¿è¾¼ã¿å±¤ã®é †ä¼æ’­ (Y = WX + B)
    N, H_prime, W_prime, C = padded_data.shape
    K, _ = conv_W.shape
    
    # Im2colå¤‰æ›: (N, H', W', C) -> (R*R*C, N*out_h*out_w)
    col = im2col(padded_data, filter_size, stride=stride, pad=0)
    
    # è¡Œåˆ—ç©ã«ã‚ˆã‚‹ç•³ã¿è¾¼ã¿è¨ˆç®—: Y = WX
    # W: (K, R*R*C) @ X: (R*R*C, N*out_h*out_w) -> Y: (K, N*out_h*out_w)
    conv_out = np.dot(conv_W, col)
    
    # ãƒã‚¤ã‚¢ã‚¹ã®åŠ ç®—
    conv_out += conv_b_vector
    
    # å‡ºåŠ›ç‰¹å¾´ãƒãƒƒãƒ—ã®å½¢çŠ¶ã«æˆ»ã™
    out_h = (H_prime - filter_size) // stride + 1 
    out_w = (W_prime - filter_size) // stride + 1 
    
    # (K, N*out_h*out_w) -> (N*out_h*out_w, K) -> (N, out_h, out_w, K)
    output = conv_out.T.reshape(N, out_h, out_w, K)
    
    return output, col

# ç•³ã¿è¾¼ã¿å±¤ã®é€†ä¼æ’­é–¢æ•°
def conv_backward(dY_4d, col):

    N, out_h, out_w, K = dY_4d.shape
    
    # dY_4d (N, out_h, out_w, K) ã‚’ dY (K, N*out_h*out_w) ã«æ•´å½¢
    dY = dY_4d.transpose(3, 0, 1, 2).reshape(K, N * out_h * out_w)
        
    dW = np.dot(dY, col.T) # (K, R*R*C)
    
    db_vector = np.sum(dY, axis=1, keepdims=True) # (K, 1)
    
    return dW, db_vector

np.random.seed(777) 

def ReLU(arr):
    """ReLUæ´»æ€§åŒ–é–¢æ•°"""
    new_arr = np.where(arr > 0, arr, 0)
    return new_arr

def softmax(x):
    """ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°"""
    alpha = np.max(x, axis=-1, keepdims=True)
    exp_x = np.exp(x - alpha)
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

# -------------------------------------------------------------
# ğŸŒŸ Max Pooling å±¤ã®è¿½åŠ 
# -------------------------------------------------------------

def max_pooling_forward(conv_output_4d, pool_h=POOL_SIZE, pool_w=POOL_SIZE, stride=POOL_STRIDE):

    N, H, W, C = conv_output_4d.shape
    out_h = (H - pool_h) // stride + 1
    out_w = (W - pool_w) // stride + 1

    # é ˜åŸŸã‚’æŠ½å‡º (N, out_h, out_w, pool_h, pool_w, C)
    # im2col ã¨åŒæ§˜ã®å‡¦ç†ã§ãƒ‘ãƒƒãƒã‚’æŠ½å‡º
    col = np.zeros((N, out_h, out_w, pool_h, pool_w, C))

    for i in range(out_h):
        i_max = i * stride + pool_h
        for j in range(out_w):
            j_max = j * stride + pool_w
            col[:, i, j, :, :, :] = conv_output_4d[:, i * stride:i_max, j * stride:j_max, :]
            
    # æœ€å¤§å€¤ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«æ•´å½¢ (N*out_h*out_w, pool_h*pool_w, C)
    col_max_calc = col.reshape(N * out_h * out_w, pool_h * pool_w, C)
    
    # æœ€å¤§å€¤ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    out_flat = np.max(col_max_calc, axis=1)    # (N*out_h*out_w, C)
    max_idx = np.argmax(col_max_calc, axis=1)  # (N*out_h*out_w, C)

    # å‡ºåŠ›å½¢çŠ¶ã«æˆ»ã™ (N, out_h, out_w, C)
    out = out_flat.reshape(N, out_h, out_w, C)
    
    return out, max_idx, conv_output_4d.shape

def max_pooling_backward(dY, max_idx, input_shape, pool_h=POOL_SIZE, pool_w=POOL_SIZE, stride=POOL_STRIDE):

    N, H, W, C = input_shape
    out_h = dY.shape[1]
    out_w = dY.shape[2]
    
    # dYã‚’ (N*out_h*out_w, C) ã«å†æ•´å½¢
    dY_flat = dY.reshape(-1, C)

    # dXã‚’Im2colã®å½¢çŠ¶ (N*out_h*out_w, pool_h*pool_w, C) ã§åˆæœŸåŒ–
    dX_col = np.zeros((dY_flat.shape[0], pool_h * pool_w, C))
    
    # max_idx (N*out_h*out_w, C)
    # dY_flat (N*out_h*out_w, C)
    
    # å‹¾é… dY_flat ã‚’æœ€å¤§å€¤ã®ä½ç½® (max_idx) ã«é…ç½®
    # np.arange(dY_flat.shape[0])[:, None] ã§ (N*out_h*out_w, 1) ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
    # np.arange(C)[None, :] ã§ (1, C) ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
    # ã“ã‚Œã‚‰ã‚’çµ„ã¿åˆã‚ã›ã¦ dX_col[è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹, åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹, ãƒãƒ£ãƒ³ãƒãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹] = å€¤ ã®å½¢å¼ã§ä»£å…¥
    
    idx_flat = np.arange(dY_flat.shape[0])[:, None]
    
    # dX_col[è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹, ç¸¦æ¨ªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹, ãƒãƒ£ãƒ³ãƒãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹] = å€¤
    dX_col[idx_flat, max_idx, np.arange(C)[None, :]] = dY_flat
    
    # dX_colã‚’ (N, out_h, out_w, pool_h, pool_w, C) ã®å½¢çŠ¶ã«æˆ»ã™
    dX_col = dX_col.reshape(N, out_h, out_w, pool_h, pool_w, C)
    
    # dXã‚’ (N, H, W, C) ã«å†æ§‹æˆ (Col2imã®é€†æ“ä½œ)
    dX = np.zeros(input_shape)
    
    for i in range(out_h):
        i_max = i * stride + pool_h
        for j in range(out_w):
            j_max = j * stride + pool_w
            # dX_colã®ãƒ‘ãƒƒãƒã‚’å…ƒã®ä½ç½®ã«åŠ ç®—
            dX[:, i * stride:i_max, j * stride:j_max, :] += dX_col[:, i, j, :, :, :]
    
    return dX

# -------------------------------------------------------------
# ğŸŒŸ é †ä¼æ’­é–¢æ•°ã®ä¿®æ­£ (Conv â†’ MaxPool â†’ ReLU â†’ FC â†’ Softmax)
# -------------------------------------------------------------

def forward_propagation(input_data_4d, conv_W, conv_b_vector, conv_R, weight2, bias2):
    """ç•³ã¿è¾¼ã¿å±¤ -> Max Pooling -> ReLU -> å…¨çµåˆå±¤2 -> Softmax ã®é †ä¼æ’­"""
    
    # ç•³ã¿è¾¼ã¿å±¤
    conv_output_pre_relu, col = conv_forward(input_data_4d, conv_W, conv_b_vector, conv_R, stride=1) 
    
    # Max Poolingå±¤
    pool_output, pool_mask, pool_input_shape = max_pooling_forward(conv_output_pre_relu)
    
    # ReLU
    relu_conv_output = ReLU(pool_output)
    
    # å…¨çµåˆå±¤ã¸ã®å…¥åŠ›ã®ãŸã‚ã«å¹³å¦åŒ–
    input_vector_fc = relu_conv_output.reshape(relu_conv_output.shape[0], -1) 
    
    # å…¨çµåˆå±¤ (å…ƒã® weight2 ãŒä½¿ç”¨ã•ã‚Œã‚‹)
    output_layer_input = np.dot(input_vector_fc, weight2.T) + bias2
    final_output = softmax(output_layer_input)
    
    # é€†ä¼æ’­ã«å¿…è¦ãªæƒ…å ±ã‚’ã¾ã¨ã‚ã¦è¿”ã™ (ã“ã“ã§ã¯ãƒ†ã‚¹ãƒˆç”¨ã®æœ€ä½é™)
    return final_output, input_vector_fc, conv_output_pre_relu, col, pool_mask, pool_input_shape, pool_output

def forward_propagation_train(input_data_4d, conv_W, conv_b_vector, conv_R, weight2, bias2, ignore_number):
    
    # ç•³ã¿è¾¼ã¿å±¤
    conv_output_pre_relu, col = conv_forward(input_data_4d, conv_W, conv_b_vector, conv_R, stride=1) 
    
    # Max Poolingå±¤
    pool_output, pool_mask, pool_input_shape = max_pooling_forward(conv_output_pre_relu)
    
    # ReLU
    relu_conv_output = ReLU(pool_output)
    
    # å…¨çµåˆå±¤ã¸ã®å…¥åŠ›ã®ãŸã‚ã«å¹³å¦åŒ–
    input_vector_fc = relu_conv_output.reshape(relu_conv_output.shape[0], -1) 
    
    # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆé©ç”¨ (å…¨çµåˆå±¤ã¸ã®å…¥åŠ›ã«é©ç”¨)
    hidden_layer_output = input_vector_fc.copy() # FCå±¤ã®å…¥åŠ›ãŒéš ã‚Œå±¤ã®å‡ºåŠ›ã«ç›¸å½“ã™ã‚‹
    for index in ignore_number:
        hidden_layer_output[:, index] = 0
        
    # å…¨çµåˆå±¤
    output_layer_input = np.dot(hidden_layer_output, weight2.T) + bias2
    final_output = softmax(output_layer_input)
    
    # é€†ä¼æ’­ã«å¿…è¦ãªæƒ…å ±ã‚’å…¨ã¦è¿”ã™
    return final_output, hidden_layer_output, conv_output_pre_relu, col, pool_mask, pool_input_shape, pool_output

def forward_propagation_test(input_data_4d, conv_W, conv_b_vector, conv_R, weight2, bias2, ignore_number):
    
    # ç•³ã¿è¾¼ã¿å±¤
    conv_output_pre_relu, col = conv_forward(input_data_4d, conv_W, conv_b_vector, conv_R, stride=1) 
    
    # Max Poolingå±¤
    pool_output, pool_mask, pool_input_shape = max_pooling_forward(conv_output_pre_relu)
    
    # ReLU
    relu_conv_output = ReLU(pool_output)
    
    # å…¨çµåˆå±¤ã¸ã®å…¥åŠ›ã®ãŸã‚ã«å¹³å¦åŒ–
    input_vector_fc = relu_conv_output.reshape(relu_conv_output.shape[0], -1) 
    
    # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨ (å…¨çµåˆå±¤ã®å…¥åŠ›ã«é©ç”¨)
    hidden_layer_output = input_vector_fc * (1 - (len(ignore_number) / fc_input_size)) # hidden_layer_size -> fc_input_size
    
    # å…¨çµåˆå±¤
    output_layer_input = np.dot(hidden_layer_output, weight2.T) + bias2
    final_output = softmax(output_layer_input)
    
    # ãƒ†ã‚¹ãƒˆæ™‚ã¯é€†ä¼æ’­æƒ…å ±ãŒä¸è¦ãªã®ã§ã€ãƒ€ãƒŸãƒ¼ã‚’è¿”ã™
    return final_output, hidden_layer_output, None, None, None, None, None

def get_predicted_class(output_probabilities):
    if output_probabilities.ndim == 1:
        return np.argmax(output_probabilities)
    else:
        return np.argmax(output_probabilities, axis=1)

def get_cross_entropy_error(y_pred, y_true):
    delta = 1e-7
    loss = -np.sum(y_true * np.log(y_pred + delta)) 
    batch_size = y_pred.shape[0]
    cross_entropy_error = loss / batch_size
    return cross_entropy_error

def get_accuracy(y_prop, y_true): 
    y_pred = get_predicted_class(y_prop) 
    accuracy = np.sum(y_pred == y_true) / len(y_prop)
    return accuracy

def calculate_accuracy_for_epoch(images, labels, conv_W, conv_b_vector, conv_R, weight2, bias2, mode, ignore_number):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ­£ç­”ç‡ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã€‚
    """
    if mode == 'train':
        probabilities, _, _, _, _, _, _ = forward_propagation_train(images, conv_W, conv_b_vector, conv_R, weight2, bias2, ignore_number)
    elif mode == 'test':
        probabilities, _, _, _, _, _, _ = forward_propagation_test(images, conv_W, conv_b_vector, conv_R, weight2, bias2, ignore_number)
    else:
        probabilities, _, _, _, _, _, _ = forward_propagation(images, conv_W, conv_b_vector, conv_R, weight2, bias2)

    accuracy = get_accuracy(probabilities, labels)

    return accuracy

# -------------------------------------------------------------
# ğŸŒŸ é€†ä¼æ’­é–¢æ•°ã®ä¿®æ­£ (å…¨çµåˆå±¤ã¯1ã¤)
# -------------------------------------------------------------

def backward_propagation_and_update_train(hidden_layer_output, output_probabilities, one_hot_labels, 
                                          weight2, bias2, learning_rate, ignore_number, momentum, prev_delta_W2,
                                          conv_output_pre_relu, col, pool_mask, pool_input_shape, pool_output):
    """
    å…¨çµåˆå±¤ã®é€†ä¼æ’­ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã€ãŠã‚ˆã³å‹¾é…è¨ˆç®—å¾Œã®ç•³ã¿è¾¼ã¿å±¤ã¸ã®èª¤å·®ä¼æ’­ã‚’è¡Œã†ã€‚
    """
    current_batch_size = hidden_layer_output.shape[0]
    
    # --- å…¨çµåˆå±¤ (weight2) ã®é€†ä¼æ’­ ---
    # èª¤å·® dEn_dak ã¯ Softmax ã®å¾Œã®å‹¾é…
    dEn_dak = (output_probabilities - one_hot_labels) / current_batch_size  # (N, Output_K)
    
    # å…¨çµåˆå±¤ã®å‹¾é…è¨ˆç®—
    dEn_dW2 = np.dot(dEn_dak.T, hidden_layer_output)  # (Output_K, FC_Input_Size)
    dEn_db2 = np.sum(dEn_dak, axis=0)                # (Output_K,)

    # èª¤å·®ã‚’å…¨çµåˆå±¤ã®å…¥åŠ› (hidden_layer_output) ã«é€†ä¼æ’­
    dEn_dX_pool = np.dot(dEn_dak, weight2)          # (N, FC_Input_Size)

    # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ã®é€†ä¼æ’­
    # hidden_layer_outputã¯ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆé©ç”¨æ¸ˆã¿ãªã®ã§ã€ä¼æ’­ã•ã‚Œã‚‹èª¤å·®ã‚‚ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã•ã‚ŒãŸå ´æ‰€ã¯ã‚¼ãƒ­
    for index in ignore_number:
         dEn_dX_pool[:, index] = 0
    
    # --- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–° ---
    delta_W2 = momentum * prev_delta_W2 - dEn_dW2 * learning_rate 
    
    weight2 += delta_W2
    bias2 -= dEn_db2 * learning_rate
    
    # --- ç•³ã¿è¾¼ã¿å±¤ã¸ä¼ãˆã‚‹èª¤å·® dY_conv_4d ã‚’è¨ˆç®— ---
    
    N = current_batch_size
    
    # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§å®šç¾©ã•ã‚ŒãŸå®šæ•°ã‚’ä½¿ç”¨
    pool_output_h = 16 
    pool_output_w = 16
    conv_K = 32
    
    # dEn_dX_pool (N, FC_Input_Size) ã‚’ 4æ¬¡å…ƒã«æˆ»ã™ (MaxPoolã®å‡ºåŠ›ã«å¯¾ã™ã‚‹å‹¾é…)
    dPool_output = dEn_dX_pool.reshape(N, pool_output_h, pool_output_w, conv_K)
    
    # ReLUå±¤ã®é€†ä¼æ’­
    dRelu_input = dPool_output * np.where(pool_output > 0, 1, 0)
    
    # Max Poolingå±¤ã®é€†ä¼æ’­ (æœ€çµ‚çš„ã« Conv å±¤ã®å‡ºåŠ›ã«å¯¾ã™ã‚‹å‹¾é… dY_conv_4d ã‚’å¾—ã‚‹)
    dY_conv_4d = max_pooling_backward(dRelu_input, pool_mask, pool_input_shape)

    return weight2, bias2, delta_W2, dY_conv_4d


# --- ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ä½¿ç”¨ã™ã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
data_dir = ensure_cifar("all/cifar-10-batches-py")
train_images, train_labels, test_images, test_labels = load_cifar10(data_dir)
# ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä»˜ä¸ (32x32 -> 34x34)
padded_train_images, padded_test_images = padding_data(train_images, test_images, pad=1)

# ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ¬¡å…ƒæ•°ã‚’ä¿®æ­£ï¼ˆCIFAR-10ç”¨ï¼‰
# input_size = 3072 Â # 32x32x3ï¼ˆã‚«ãƒ©ãƒ¼ç”»åƒï¼‰
# hidden_layer_size = 100 Â # ä¸­é–“å±¤ã¯ä¸è¦ã ãŒã€ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®å…¥åŠ›ã‚µã‚¤ã‚ºã¨ã—ã¦åˆ©ç”¨
output_layer_size = 10 # CIFAR-10ã‚‚10ã‚¯ãƒ©ã‚¹

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if __name__ == "__main__":
    
    # ç•³ã¿è¾¼ã¿å±¤ã®å‡ºåŠ›ã‚µã‚¤ã‚ºè¨ˆç®—
    conv_K = 32 # ãƒ•ã‚£ãƒ«ã‚¿æšæ•°
    conv_output_h = 32 
    conv_output_w = 32
    pool_output_h = (conv_output_h - POOL_SIZE) // POOL_STRIDE + 1 # 16
    pool_output_w = (conv_output_w - POOL_SIZE) // POOL_STRIDE + 1 # 16
    fc_input_size = pool_output_h * pool_output_w * conv_K # 16 * 16 * 32 = 8192
    
    # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®ã‚µã‚¤ã‚ºã¨ã—ã¦fc_input_sizeã‚’ä½¿ç”¨
    hidden_layer_size = fc_input_size 
    
    batch_size = 100
    epoch_number = 10
    learning_rate = 0.01
    train_loss_list, train_acc_list, test_acc_list = [], [], []
    momentum = 0.9
    
    # weight1, prev_delta_W1 ã¯ä¸è¦ã«ãªã£ãŸãŸã‚ã€weight2, prev_delta_W2 ã®ã¿ä½¿ç”¨
    prev_delta_W2 = 0 

    is_load = str(input('ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ yes or no: '))
    if is_load == 'yes' :
        # ãƒ­ãƒ¼ãƒ‰å‡¦ç† (ãƒ•ã‚¡ã‚¤ãƒ«åæ³¨æ„)
        loaded_data = np.load('assignment6_parameter.npz')
        weight2 = loaded_data['weight2']
        bias2 = loaded_data['bias2']
        conv_W = loaded_data['conv_W']
        conv_b_vector = loaded_data['conv_b_vector']
    else:
        # é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã‚’æ­£è¦åˆ†å¸ƒã§åˆæœŸåŒ–
        conv_W, conv_R = set_filter_weights() # conv_W: (32, 27) conv_R: 3(ãƒ•ã‚£ãƒ«ã‚¿ã‚µã‚¤ã‚º)
        conv_b_vector = set_biases() # conv_b_vector: (32, 1)
        
        # å…¨çµåˆå±¤ï¼ˆä¸­é–“å±¤ -> å‡ºåŠ›å±¤ã ã£ãŸã‚‚ã®ãŒã€Convå‡ºåŠ› -> å‡ºåŠ›å±¤ã«å¤‰æ›´ï¼‰
        weight2 = np.random.normal(loc=0.0, scale=np.sqrt(1 / fc_input_size), size=(output_layer_size, fc_input_size)) # (10, 8192)
        bias2 = np.random.normal(loc=0.0, scale=np.sqrt(1 / fc_input_size), size=output_layer_size) # (10,)

    while True:
        mode = str(input('å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (train or test): '))
        if mode in ['train', 'test']:
            break
        print("ç„¡åŠ¹ãªãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚'train' ã¾ãŸã¯ 'test' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    while True:
        try:
            # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®å€‹æ•°ã‚’ fc_input_size (8192) ã®ç¯„å›²ã§å…¥åŠ›
            ignore_number = int(input(f'Dropoutã®å€‹æ•°ã‚’ 0 ~ {fc_input_size} ã§å…¥åŠ›ã—ã¦ãã ã•ã„: '))
            if 0 <= ignore_number <= fc_input_size:
                break
            else:
                print(f"ç„¡åŠ¹ãªãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆæ•°ã§ã™ã€‚0ã‹ã‚‰{fc_input_size}ã®ç¯„å›²ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        except ValueError:
            print("ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚æ•´æ•°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã«ã®ã¿å­¦ç¿’ã‚’å®Ÿè¡Œ
    if mode == 'train':
        print("\n--- è¨“ç·´ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œä¸­ ---")

        for i in range(1, epoch_number + 1):
            error_sum = 0
            train_accuracy_sum = 0
            shuffled_train_image_index = get_shuffled_index(train_images)
            
            for j in range(0, len(shuffled_train_image_index), batch_size): 

                # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ FC_Input_Size (8192) ã®ç¯„å›²ã§é¸æŠ
                random_selection = np.random.choice(np.arange(fc_input_size), size=ignore_number, replace=False)
                index = shuffled_train_image_index[j:j + batch_size] 

                batch_image, batch_labels = get_batch(index) # â˜…ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã®ä¸Šæ›¸ãã‚’é˜²ããŸã‚å¤‰æ•°åã‚’ä¿®æ­£
                
                # --- é †ä¼æ’­ ---
                output_probabilities, hidden_layer_output, conv_output_pre_relu, train_images_col, pool_mask, pool_input_shape, pool_output = forward_propagation_train(
                    batch_image, conv_W, conv_b_vector, conv_R, 
                    weight2, bias2, random_selection
                )
                
                one_hot_labels = get_one_hot_label(batch_labels, output_layer_size)
                calculated_error = get_cross_entropy_error(output_probabilities, one_hot_labels)
                error_sum += calculated_error
                
                # --- é€†ä¼æ’­ ---
                # å…¨çµåˆå±¤ã®é€†ä¼æ’­ã¨æ›´æ–°ã€‚Convå±¤ã¸ä¼æ’­ã•ã›ã‚‹èª¤å·® dY_conv_4d ã‚’å–å¾—
                weight2, bias2, prev_delta_W2, dY_conv_4d = backward_propagation_and_update_train(
                    hidden_layer_output, # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆé©ç”¨æ¸ˆã¿ã® FC å±¤ã¸ã®å…¥åŠ› (N, FC_Input_Size)
                    output_probabilities, one_hot_labels,
                    weight2, bias2, learning_rate, random_selection, momentum, prev_delta_W2,
                    conv_output_pre_relu, train_images_col, pool_mask, pool_input_shape, pool_output
                )
                
                # Convå±¤ã®é€†ä¼æ’­ã¨æ›´æ–°
                
                # Max Pooling ã®é€†ä¼æ’­ã§å¾—ã‚‰ã‚ŒãŸå‹¾é… dY_conv_4d ã¯ Conv ã®å‡ºåŠ›ã«å¯¾ã™ã‚‹å‹¾é…
                # ã“ã‚Œã« Conv å¾Œã®æ´»æ€§åŒ–é–¢æ•° (ReLU) ã®å¾®åˆ†ã‚’ã‹ã‘ã‚‹å¿…è¦ãŒã‚ã‚‹
                
                # (Max Poolingã®å¾Œã«ReLUãŒã‚ã‚‹ãŸã‚ã€Conv ã®å‡ºåŠ› pre-ReLU ã®å‹¾é…ã‚’æ±‚ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹)
                # Max Pooling ã®å…¥åŠ›ã¯ Conv ã®å‡ºåŠ› pre-ReLU ã§ã¯ãªã„ãŸã‚ã€dY_conv_4d ã¯ Conv ã®å‡ºåŠ› pre-ReLU ã®å‹¾é…ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹
                
                # ã“ã“ã§ã¯ã€ç°¡ç•¥åŒ–ã®ãŸã‚ã€Conv ã®å‡ºåŠ› pre-ReLU ã®å‹¾é…ã¨ã—ã¦æ‰±ã†
                
                # Conv-ReLU ã®é€†ä¼æ’­ (ReLU(MaxPool) ã®å‰ã®å±¤)
                # dY_conv_4d ã¯ MaxPool_Backward ã®å‡ºåŠ›ã§ã‚ã‚Šã€Conv å±¤ã®å‡ºåŠ›ã«å¯¾ã™ã‚‹å‹¾é… (dLoss/dConv_Out) ã«ç›¸å½“ã™ã‚‹
                relu_diff_conv = np.where(conv_output_pre_relu > 0, 1, 0)
                dY_conv_4d *= relu_diff_conv
                
                # train_images_col ã¯é †ä¼æ’­ã§å¾—ã‚‰ã‚ŒãŸ im2col ã®çµæœ (X)
                dW, db_vector = conv_backward(dY_conv_4d, train_images_col)
                
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
                conv_W -= dW * learning_rate
                conv_b_vector -= db_vector * learning_rate # db_vectorã¯(32,1)ãªã®ã§ãã®ã¾ã¾æ¸›ç®—
                
                
                train_accuracy_sum += calculate_accuracy_for_epoch(
                    batch_image, batch_labels, conv_W, conv_b_vector, conv_R, 
                    weight2, bias2, 'train', random_selection
                )
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ç²¾åº¦è¨ˆç®—
            ignore_index_for_acc = np.arange(fc_input_size)[:ignore_number] 
            
            test_accuracy = calculate_accuracy_for_epoch(
                padded_test_images, test_labels, conv_W, conv_b_vector, conv_R,
                weight2, bias2, 'test', ignore_index_for_acc
            )
            
            num_batches = len(train_images) // batch_size
            train_loss_list.append(error_sum / num_batches)
            train_acc_list.append(train_accuracy_sum/ num_batches)
            test_acc_list.append(test_accuracy)
            print(f"{i}ã‚¨ãƒãƒƒã‚¯ç›®")
            print(f" å¹³å‡ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼èª¤å·®: {error_sum / num_batches}")
            print(f" å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ­£ç­”ç‡: {train_accuracy_sum / num_batches}")
            print(f" ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ­£ç­”ç‡: {test_accuracy}")
        
        # --- ã‚°ãƒ©ãƒ•ã®æç”» ---
        x = np.arange(1, epoch_number + 1)
        plt.figure(figsize=(12, 5))

        # èª¤å·®ã®ã‚°ãƒ©ãƒ•
        plt.subplot(1, 2, 1)
        plt.plot(x, train_loss_list, marker='o')
        plt.title('Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True)

        # æ­£ç­”ç‡ã®ã‚°ãƒ©ãƒ•
        plt.subplot(1, 2, 2)
        plt.plot(x, train_acc_list, marker='o', label='Train Accuracy')
        plt.plot(x, test_acc_list, marker='s', label='Test Accuracy')
        plt.title('Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.ylim(0, 1.0)
        plt.grid(True)
        plt.legend()

        plt.tight_layout()
        plt.show()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¿å­˜
        np.savez('assignment6_parameter.npz', weight2 = weight2, bias2 = bias2, conv_W = conv_W, conv_b_vector = conv_b_vector)

    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã«ã®ã¿äºˆæ¸¬ã‚’å®Ÿè¡Œ
    elif mode == 'test':
        print("\n--- ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œä¸­ Â ---")
        random_selection = np.random.choice(np.arange(fc_input_size), size=ignore_number, replace=False) # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®ã‚µã‚¤ã‚ºã‚’ fc_input_size ã«ä¿®æ­£
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æœ€çµ‚çš„ãªæ­£ç­”ç‡ã‚’è¨ˆç®—
        test_accuracy = calculate_accuracy_for_epoch(
            padded_test_images, test_labels, conv_W, conv_b_vector, conv_R,
            weight2, bias2, 'test', random_selection
        )
        
        print(f"\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æœ€çµ‚æ­£ç­”ç‡: {test_accuracy}")
        print(f"ï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆæ•° {ignore_number} å€‹ï¼‰")